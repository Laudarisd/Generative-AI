# Activation Functions in Machine Learning

Activation functions introduce nonlinearity into neural networks, enabling them to model complex relationships. Here’s a summary of the most widely used functions.

---

## Table of Contents

- [Sigmoid (Logistic)](#sigmoid-logistic)
- [Tanh (Hyperbolic Tangent)](#tanh-hyperbolic-tangent)
- [ReLU (Rectified Linear Unit)](#relu-rectified-linear-unit)
- [Leaky ReLU](#leaky-relu)
- [Parametric ReLU (PReLU)](#parametric-relu-prelu)
- [ELU (Exponential Linear Unit)](#elu-exponential-linear-unit)
- [SELU (Scaled ELU)](#selu-scaled-elu)
- [Swish](#swish)
- [Softmax](#softmax)
- [Hard Sigmoid, Hard Tanh](#hard-sigmoid-hard-tanh)
- [Other/Modern Activations](#othermodern-activations)
- [References](#references)

---

## Sigmoid (Logistic)

- **Formula:**
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$
- **Range:** $(0, 1)$
- **Pros:** Good for probability outputs (binary classification)
- **Cons:** Vanishing gradient for large $|x|$; not zero-centered

---

## Tanh (Hyperbolic Tangent)

- **Formula:**
  $$
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  $$
- **Range:** $(-1, 1)$
- **Pros:** Zero-centered, better than sigmoid for many tasks
- **Cons:** Still suffers from vanishing gradients

---

## ReLU (Rectified Linear Unit)

- **Formula:**
  $$
  \text{ReLU}(x) = \max(0, x)
  $$
- **Range:** $[0, \infty)$
- **Pros:** Sparse activation, efficient, fast to compute
- **Cons:** “Dying ReLU” (neurons can get stuck at 0)

---

## Leaky ReLU

- **Formula:**
  $$
  \text{LeakyReLU}(x) = 
  \begin{cases}
    x, & x \geq 0 \\
    \alpha x, & x < 0
  \end{cases}
  $$

  where typically $\alpha = 0.01$
- **Pros:** Solves “dying ReLU” by allowing a small negative slope

---

## Parametric ReLU (PReLU)

- **Formula:**
  $$
  \text{PReLU}(x) = 
  \begin{cases}
    x, & x \geq 0 \\
    a x, & x < 0
  \end{cases}
  $$

  where $a$ is a learned parameter

---

## ELU (Exponential Linear Unit)

- **Formula:**

  $$
  \text{ELU}(x) = 
  \begin{cases}
    x, & x \geq 0 \\
    \alpha (e^{x} - 1), & x < 0
  \end{cases}
  $$

  typically $\alpha = 1.0$
- **Pros:** Reduces bias shift, allows negative outputs

---

## SELU (Scaled Exponential Linear Unit)

- **Formula:**

  $$
  \text{SELU}(x) = \lambda
  \begin{cases}
    x, & x > 0 \\
    \alpha (e^{x} - 1), & x \leq 0
  \end{cases}
  $$

  with fixed $\lambda \approx 1.0507$, $\alpha \approx 1.6733$
- **Pros:** Self-normalizing, works well with special initialization and network architecture

---

## Swish

- **Formula:**

  $$
  \text{Swish}(x) = x \cdot \sigma(\beta x)
  $$

  where $\beta$ is usually 1; $\sigma$ is the sigmoid function
- **Pros:** Smooth, non-monotonic, sometimes outperforms ReLU in deep networks

---

## Softmax

- **Formula:**
  $$
  \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
  $$
- **Usage:** Converts vector to probability distribution (multi-class classification, last layer)

---

## Hard Sigmoid, Hard Tanh

- **Hard Sigmoid:** Piecewise linear approximation of sigmoid.
- **Hard Tanh:** Piecewise linear approximation of tanh.

---

## Other/Modern Activations

- **GELU (Gaussian Error Linear Unit):**

  $$
  \text{GELU}(x) = x \cdot \Phi(x)
  $$

  where $\Phi(x)$ is the standard normal cumulative distribution function (approx: $0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$)

  - Used in BERT, Transformers.
- **Mish:** $x \cdot \tanh(\ln(1 + e^x))$

  - Smooth, non-monotonic; good empirical performance.

---

## References

- [Stanford CS231n: Activation Functions](https://cs231n.github.io/neural-networks-1/#actfun)
- [PyTorch Activation Docs](https://pytorch.org/docs/stable/nn.html#non-linear-activations)
- [Keras Activation Docs](https://keras.io/api/layers/activations/)
- [GELU Paper (Hendrycks &amp; Gimpel, 2016)](https://arxiv.org/abs/1606.08415)
