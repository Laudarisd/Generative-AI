# Loss Functions in Machine Learning

Loss (or cost) functions measure how well a model’s predictions match the target values. The choice of loss affects model training, convergence, and final performance.

---

## Table of Contents

- [Regression Losses](#regression-losses)
  - [Mean Squared Error (MSE)](#mean-squared-error-mse)
  - [Mean Absolute Error (MAE)](#mean-absolute-error-mae)
  - [Huber Loss](#huber-loss)
  - [Log-Cosh Loss](#log-cosh-loss)
- [Classification Losses](#classification-losses)
  - [Binary Cross-Entropy (Log Loss)](#binary-cross-entropy-log-loss)
  - [Categorical Cross-Entropy](#categorical-cross-entropy)
  - [Kullback-Leibler (KL) Divergence](#kullback-leibler-kl-divergence)
  - [Hinge Loss (SVM)](#hinge-loss-svm)
  - [Focal Loss](#focal-loss)
- [Advanced and Specialized Losses](#advanced-and-specialized-losses)
  - [Dice Loss](#dice-loss)
  - [IoU Loss (Jaccard Loss)](#iou-loss-jaccard-loss)
  - [Triplet Loss](#triplet-loss)
  - [Contrastive Loss](#contrastive-loss)
  - [CTC Loss](#ctc-loss)
  - [Custom/Composite Losses](#customcomposite-losses)
- [Choosing a Loss Function](#choosing-a-loss-function)
- [References](#references)

---

## Regression Losses

### Mean Squared Error (MSE)

- **Formula:**
  $$
  \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
  $$
- **Usage:** Most common for regression. Penalizes larger errors more.
- **Notes:** Sensitive to outliers.

---

### Mean Absolute Error (MAE)

- **Formula:**
  $$
  \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
  $$
- **Usage:** Regression, robust to outliers (less than MSE).
- **Notes:** Not differentiable at $0$; gradient is constant ($+1$ or $-1$).

---

### Huber Loss

- **Combines MSE and MAE; less sensitive to outliers than MSE.**
- **Formula:**
  $$
  L_\delta(y, \hat{y}) = 
    \begin{cases}
      \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| < \delta \\
      \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
    \end{cases}
  $$
- **Usage:** Regression, especially when outliers are present.

---

### Log-Cosh Loss

- **Formula:**
  $$
  L(y, \hat{y}) = \sum \log\left(\cosh(\hat{y} - y)\right)
  $$
- **Usage:** Regression; smooth version of MAE.

---

## Classification Losses

### Binary Cross-Entropy (Log Loss)

- **Formula (binary):**
  $$
  L(y, \hat{y}) = - [y \log(\hat{y}) + (1-y) \log(1 - \hat{y})]
  $$
- **Usage:** Binary classification, logistic regression, neural nets.
- **Notes:** Output $\hat{y}$ must be in $(0,1)$ (use sigmoid).

---

### Categorical Cross-Entropy

- **Formula (multi-class):**
  $$
  L(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{c=1}^C y_c \log(\hat{y}_c)
  $$

  where $C$ is number of classes, $y_c$ is one-hot target.
- **Usage:** Multi-class classification (softmax output).
- **Notes:** Generalizes binary cross-entropy.

---

### Kullback-Leibler (KL) Divergence

- **Formula:**
  $$
  D_{KL}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}
  $$
- **Usage:** Measuring difference between two probability distributions (e.g., distillation, VAEs).
- **Notes:** Not symmetric.

---

### Hinge Loss (SVM)

- **Formula:**
  $$
  L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})
  $$

  (For $y \in \{-1, +1\}$)
- **Usage:** Support Vector Machines, margin-based classifiers.

---

### Focal Loss

- **Formula:**
  $$
  FL(p_t) = -\alpha (1-p_t)^\gamma \log(p_t)
  $$

  where $p_t$ is the predicted probability for the correct class, $\gamma$ (focusing parameter), $\alpha$ (balancing).
- **Usage:** Addressing class imbalance in detection/classification tasks.

---

## Advanced and Specialized Losses

### Dice Loss

- **Formula:**
  $$
  L_{Dice} = 1 - \frac{2 |X \cap Y|}{|X| + |Y|}
  $$
- **Usage:** Image segmentation (measures overlap between prediction and ground truth masks).

---

### IoU Loss (Jaccard Loss)

- **Formula:**
  $$
  L_{IoU} = 1 - \frac{|X \cap Y|}{|X \cup Y|}
  $$
- **Usage:** Segmentation, object detection.

---

### Triplet Loss

- **Formula:**
  $$
  L(A, P, N) = \max(0, d(A,P) - d(A,N) + \alpha)
  $$

  where $A$=anchor, $P$=positive, $N$=negative, $\alpha$=margin.
- **Usage:** Metric learning, face recognition (encourage anchor closer to positive than negative by margin).

---

### Contrastive Loss

- **Formula:**
  $$
  L = (1 - Y) \frac{1}{2} (D_W)^2 + (Y) \frac{1}{2} \max(0, m - D_W)^2
  $$

  where $D_W$ is the distance between pairs, $Y$=1 for similar, $Y$=0 for dissimilar, $m$=margin.
- **Usage:** Siamese networks, metric learning.

---

### CTC Loss (Connectionist Temporal Classification)

- **Usage:** Sequence prediction (speech, handwriting recognition) where alignment between input/output is unknown.
- **Intuition:** Allows neural nets to predict unaligned sequences (labeling at the sequence level, not per-frame).
- **Complex recursive formulation—use library implementations.**

---

### Custom/Composite Losses

- Combine multiple losses for multi-task learning or to balance different objectives.
  E.g., $\mathrm{Loss} = \mathrm{CrossEntropy} + \lambda \times \mathrm{Dice}$

---

## Choosing a Loss Function

- **Regression:** Use MSE, MAE, or Huber (depending on outliers).
- **Classification:** Use Cross-Entropy (BCE/CE), sometimes Focal or Hinge Loss.
- **Segmentation:** Use Dice, IoU, or BCE+Dice.
- **Metric Learning:** Triplet or Contrastive Loss.
- **Probabilistic Outputs:** KL Divergence.

---

## References

- [PyTorch Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [Deep Learning Book (Goodfellow et al.)](https://www.deeplearningbook.org/)
- [Wikipedia: Loss Function](https://en.wikipedia.org/wiki/Loss_function)
- [Sebastian Raschka&#39;s ML Loss Function Summary](https://sebastianraschka.com/pdf/lecture-notes/stat479su20/L11-loss-functions__annotated.pdf)
