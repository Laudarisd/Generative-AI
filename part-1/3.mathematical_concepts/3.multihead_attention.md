## Multi-Head Attention: Mathematical Concepts

Multi-head attention is a core mechanism in transformer models that allows the model to focus on different parts of the input sequence in parallel, capturing a range of relationships and contexts.

### Mathematical Formulation

Given an input matrix $X \in \mathbb{R}^{n \times d_{\text{model}}}$ (where $n$ = sequence length, $d_{\text{model}}$ = embedding size):

For each attention head $i = 1, \ldots, h$:

- Learn projection matrices:
  - $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$
  - $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$
  - $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$

For head $i$:

- $Q_i = X W_i^Q$
- $K_i = X W_i^K$
- $V_i = X W_i^V$

The attention weights and output for each head are computed as:

$$
\text{Attention}_i(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i
$$

**Multi-head concatenation:**

$$
\text{MultiHead}(X) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

where $W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}$.

---

### Example: Numerical Multi-Head Attention

Suppose:

- Sequence length $n = 2$
- Model dimension $d_{\text{model}} = 4$
- Number of heads $h = 2$
- Each head uses $d_k = d_v = 2$

#### Step 1: Inputs

Let $X$ be the input embeddings (batch size = 1 for simplicity):

$$
X = \begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 2 & 0 & 2 \\
\end{bmatrix}
$$

#### Step 2: Projection Matrices (random example)

For head 1:

- $W_1^Q = W_1^K = W_1^V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$

For head 2:

- $W_2^Q = W_2^K = W_2^V = \begin{bmatrix} 0 & 1 \\ 1 & 0 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}$

#### Step 3: Compute Queries, Keys, Values

For head 1:

- $Q_1 = X W_1^Q$
- $K_1 = X W_1^K$
- $V_1 = X W_1^V$

For our $X$ and $W_1^Q$:

$$
Q_1 = \begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 2 & 0 & 2 \\
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 0 \\
0 & 1 \\
\end{bmatrix}
= 
\begin{bmatrix}
(1+1) & (0+0) \\
(0+0) & (2+2) \\
\end{bmatrix}
=
\begin{bmatrix}
2 & 0 \\
0 & 4 \\
\end{bmatrix}
$$

(Similarly for $K_1$ and $V_1$.)

#### Step 4: Attention Weights

$$
\text{Scores} = \frac{Q_1 K_1^T}{\sqrt{2}}
$$

Compute softmax row-wise, then multiply by $V_1$ for the attended outputs.

#### Step 5: Multi-Head Concatenation

- Concatenate the outputs of both heads, project through $W^O$ to return to $d_{\text{model}}$.

---

### Intuitive Summary

- **Multi-head** = Multiple sets of (Q, K, V) projections to capture information from different representation subspaces.
- Each head attends to different relationships in the sequence.
- Outputs are concatenated and projected to allow information sharing between heads.

---

### References

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
