44


# Optimization Algorithms in Machine Learning

Optimization algorithms are fundamental to machine learning—they’re used to **minimize (or maximize) a loss function** during model training. Below are the most common algorithms, their formulas, explanations, and practical notes.

---

## Table of Contents

- [Why Optimization?](#why-optimization)
- [Gradient Descent Family](#gradient-descent-family)
  - [Batch Gradient Descent (BGD)](#batch-gradient-descent-bgd)
  - [Stochastic Gradient Descent (SGD)](#stochastic-gradient-descent-sgd)
  - [Mini-Batch Gradient Descent](#mini-batch-gradient-descent)
  - [Momentum](#momentum)
  - [Nesterov Accelerated Gradient (NAG)](#nesterov-accelerated-gradient-nag)
  - [Adagrad](#adagrad)
  - [RMSProp](#rmsprop)
  - [Adam](#adam)
  - [Other Adaptive Methods: Adadelta, Nadam, AMSGrad](#other-adaptive-methods)
- [Second-Order Methods](#second-order-methods)
  - [Newton’s Method](#newtons-method)
  - [Quasi-Newton (L-BFGS, BFGS)](#quasi-newton-l-bfgs-bfgs)
- [Specialized/Heuristic Methods](#specializedheuristic-methods)
  - [Genetic Algorithms (GA)](#genetic-algorithms-ga)
  - [Simulated Annealing](#simulated-annealing)
  - [Swarm Optimization (PSO, etc)](#swarm-optimization-pso-etc)
- [Practical Tips for Choosing an Optimizer](#practical-tips-for-choosing-an-optimizer)
- [References](#references)

---

## Why Optimization?

- **Goal:** Minimize a loss/cost function $L(\theta)$ over parameters $\theta$ (e.g., weights in neural networks)
- **Typical Losses:** Mean squared error (regression), cross-entropy (classification), negative log-likelihood, etc.
- **Most optimization in ML is unconstrained and high-dimensional.**

---

## Gradient Descent Family

### Batch Gradient Descent (BGD)

- **Update Rule:**$\theta := \theta - \eta \nabla_\theta L(\theta)$(where $\eta$ = learning rate, $\nabla_\theta L(\theta)$ = gradient)
- **Computes gradient over the entire dataset for each update.**
- **Pros:** Stable convergence.
  **Cons:** Slow for large datasets; can get stuck in local minima.

---

### Stochastic Gradient Descent (SGD)

- **Update Rule:**$\theta := \theta - \eta \nabla_\theta L_i(\theta)$(where $L_i$ = loss for one sample)
- **Updates parameters for each training example.**
- **Pros:** Faster, can escape local minima.
- **Cons:** Noisy updates; loss may “bounce.”

---

### Mini-Batch Gradient Descent

- **Update Rule:** Like SGD but uses a batch of $m$ samples.
- **Most popular in deep learning (batch size typically 32–512).**
- **Tradeoff between speed and stability.**

---

### Momentum

- **Update Rule:**$v := \gamma v + \eta \nabla_\theta L(\theta)$$\theta := \theta - v$($\gamma$ = momentum parameter, usually $0.9$)
- **Accelerates convergence by dampening oscillations.**

---

### Nesterov Accelerated Gradient (NAG)

- **Lookahead step:** Compute gradient at “future” location.
- **Update:**$v := \gamma v + \eta \nabla_\theta L(\theta - \gamma v)$$\theta := \theta - v$
- **Faster convergence than classic momentum in practice.**

---

### Adagrad

- **Adaptively scales learning rates for each parameter.**
- **Update Rule:**$g_t := \nabla_\theta L(\theta_t)$$G_t := G_{t-1} + g_t^2$ (elementwise)$\theta_{t+1} := \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t$
- **Pros:** No manual learning rate tuning.
- **Cons:** Accumulated squared gradients can make learning rate too small.

---

### RMSProp

- **Fixes Adagrad’s decaying learning rate.**
- **Update Rule:**$E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2$$\theta_{t+1} := \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$
- **Works well for RNNs and deep nets.**

---

### Adam (Adaptive Moment Estimation)

- **Combines Momentum and RMSProp.**
- **Update Rule:**$m_t := \beta_1 m_{t-1} + (1 - \beta_1)g_t$$v_t := \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$ (bias-corrected)$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$\theta_{t+1} := \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$
- **Default for deep learning.** $\beta_1 = 0.9, \beta_2 = 0.999$

---

### Other Adaptive Methods

- **Adadelta:** Variant of Adagrad, no need for manual learning rate.
- **Nadam:** Adam + Nesterov momentum.
- **AMSGrad:** Modification of Adam to improve theoretical convergence.

---

## Second-Order Methods

### Newton’s Method

- **Uses the Hessian matrix ($H$) of second derivatives:**$\theta := \theta - H^{-1} \nabla_\theta L(\theta)$
- **Very fast convergence but computation of $H$ is expensive in high dimensions.**

---

### Quasi-Newton (L-BFGS, BFGS)

- **Approximate Hessian instead of computing it directly.**
- **L-BFGS** is memory-efficient (limited memory), used for large-scale problems.
- **Often used for classical ML (SVMs, logistic regression) but not deep nets.**

---

## Specialized/Heuristic Methods

### Genetic Algorithms (GA)

- **Population-based optimization inspired by natural selection.**
- **Operations:** Selection, Crossover, Mutation.
- **Used for hyperparameter tuning, neural architecture search.**
- **Pros:** Black-box, no gradient required.

---

### Simulated Annealing

- **Probabilistic technique that escapes local minima by random walks.**
- **Analogy:** Cooling process in metallurgy; gradually reduces "temperature" to settle in global minimum.
- **Useful for combinatorial and discrete optimization.**

---

### Swarm Optimization (PSO, etc)

- **Particle Swarm Optimization (PSO):** Population of particles explore the search space using their own and neighbors' best positions.
- **Other Swarm Methods:** Ant Colony, Firefly, etc.
- **Applications:** Black-box optimization, feature selection.

---

## Practical Tips for Choosing an Optimizer

- **Adam/RMSProp:** Default for deep learning, good for most architectures.
- **SGD + Momentum:** Often yields better generalization (esp. for CNNs), but may need more tuning.
- **Second-Order (L-BFGS):** Use for small/medium datasets and convex problems.
- **Black-box methods (GA, PSO):** Use for non-differentiable, discrete, or very rugged landscapes.

**Always:**

- Tune the learning rate!
- Consider batch size, as it affects optimization dynamics.
- Monitor training & validation loss for overfitting or underfitting.

---

## References

- [Stanford CS231n Optimization Notes](https://cs231n.github.io/optimization-1/)
- [Sebastian Ruder: An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
- [Goodfellow et al., Deep Learning (Book)](https://www.deeplearningbook.org/)
- [PyTorch Optimizers Documentation](https://pytorch.org/docs/stable/optim.html)
- [Wikipedia: List of optimization algorithms](https://en.wikipedia.org/wiki/List_of_optimization_algorithms)
