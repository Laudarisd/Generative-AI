# Linear Algebra Cheat Sheet

## Table of Contents

- [Vectors &amp; Vector Operations](#vectors--vector-operations)
- [Vector Spaces &amp; Subspaces](#vector-spaces--subspaces)
- [Matrices &amp; Matrix Operations](#matrices--matrix-operations)
- [Determinants](#determinants)
- [Systems of Linear Equations](#systems-of-linear-equations)
- [Linear Transformations](#linear-transformations)
- [Eigenvalues &amp; Eigenvectors](#eigenvalues--eigenvectors)
- [Diagonalization &amp; Jordan Form](#diagonalization--jordan-form)
- [Inner Product Spaces &amp; Orthogonality](#inner-product-spaces--orthogonality)
- [Gram-Schmidt &amp; QR Factorization](#gram-schmidt--qr-factorization)
- [Orthogonal &amp; Unitary Matrices](#orthogonal--unitary-matrices)
- [Spectral Theorem &amp; Singular Value Decomposition (SVD)](#spectral-theorem--singular-value-decomposition-svd)
- [Rank, Nullity, &amp; The Fundamental Theorem](#rank-nullity--the-fundamental-theorem)
- [Advanced Topics](#advanced-topics)
- [References](#references)

---

## Vectors & Vector Operations

- **Vector:** Ordered list $(v_1, v_2, ..., v_n)$.
- **Addition:** $\vec{u} + \vec{v} = (u_1+v_1, ..., u_n+v_n)$
- **Scalar Multiplication:** $a\vec{v} = (av_1, ..., av_n)$
- **Dot Product:** $\vec{u} \cdot \vec{v} = \sum u_i v_i$
- **Norm (Length):** $||\vec{v}|| = \sqrt{\vec{v} \cdot \vec{v}}$
- **Cross Product:** (in $\mathbb{R}^3$) $\vec{u} \times \vec{v}$

---

## Vector Spaces & Subspaces

- **Vector Space:** Closed under addition and scalar multiplication.
- **Subspace:** A subset that is a vector space.
- **Span:** Set of all linear combinations.
- **Basis:** Minimal spanning set of linearly independent vectors.
- **Dimension:** Number of vectors in a basis.

---

## Matrices & Matrix Operations

- **Matrix $A$:** $m \times n$ array, $A \in \mathbb{R}^{m \times n}$.
- **Transpose:** $A^T$
- **Addition & Scalar Multiplication:** Entrywise.
- **Matrix Multiplication:** $(AB)_{ij} = \sum_{k} A_{ik}B_{kj}$
- **Identity Matrix:** $I_n$, $I_n x = x$
- **Inverse:** $A^{-1}$, $AA^{-1} = I$ (if $A$ is square and non-singular)

---

## Determinants

- **Definition:** $\det(A)$, scalar value for square matrices.
- **Properties:**
  - $\det(AB) = \det(A)\det(B)$
  - $\det(A^T) = \det(A)$
  - $\det(cA) = c^n\det(A)$ for $A \in \mathbb{R}^{n \times n}$
- **Geometric Interpretation:** Volume scaling factor.
- **Singular/Non-singular:** $A$ invertible iff $\det(A) \neq 0$

---

## Systems of Linear Equations

- **Standard Form:** $A\vec{x} = \vec{b}$
- **Row Echelon Form & Reduced Row Echelon Form (RREF)**
- **Gaussian/Gauss-Jordan Elimination:** Algorithm for solving systems.
- **Consistency:** Unique, infinite, or no solution (check rank).

---

## Linear Transformations

- **Definition:** $T: V \to W$ linear if $T(a\vec{x} + b\vec{y}) = aT(\vec{x}) + bT(\vec{y})$
- **Matrix Representation:** $T(\vec{x}) = A\vec{x}$
- **Kernel (Null Space):** $\ker(T) = \{\vec{x}: T(\vec{x}) = 0\}$
- **Image (Column Space):** $\mathrm{Im}(T) = \{T(\vec{x})\}$

---

## Eigenvalues & Eigenvectors

- **Eigenvalue Equation:** $A\vec{v} = \lambda\vec{v}$, $\vec{v} \neq 0$
- **Characteristic Polynomial:** $\det(A - \lambda I) = 0$
- **Algebraic & Geometric Multiplicity**
- **Eigenspace:** Set of all eigenvectors for $\lambda$ plus zero vector.

---

## Diagonalization & Jordan Form

- **Diagonalizable:** $A = PDP^{-1}$, $D$ diagonal.
- **When Diagonalizable:** $A$ has $n$ independent eigenvectors.
- **Jordan Canonical Form:** $A = PJP^{-1}$, $J$ block diagonal with Jordan blocks.

---

## Inner Product Spaces & Orthogonality

- **Inner Product:** $\langle \vec{u}, \vec{v} \rangle = \vec{u}^T \vec{v}$
- **Norm:** $||\vec{v}|| = \sqrt{\langle \vec{v}, \vec{v} \rangle}$
- **Orthogonality:** $\langle \vec{u}, \vec{v} \rangle = 0$
- **Orthonormal Set:** Orthogonal and all vectors have unit norm.

---

## Gram-Schmidt & QR Factorization

- **Gram-Schmidt Process:** Converts basis to orthonormal basis.
- **QR Factorization:** $A = QR$, $Q$ orthonormal, $R$ upper triangular.

---

## Orthogonal & Unitary Matrices

- **Orthogonal Matrix:** $Q^T Q = QQ^T = I$
- **Unitary Matrix (Complex):** $U^* U = I$ ($U^*$ is conjugate transpose)
- **Properties:** Preserve lengths and angles, $|\det(Q)| = 1$

---

## Spectral Theorem & Singular Value Decomposition (SVD)

- **Spectral Theorem:** Any symmetric matrix $A$ can be diagonalized by orthogonal matrix: $A = Q\Lambda Q^T$.
- **SVD:** For $A \in \mathbb{R}^{m \times n}$, $A = U \Sigma V^T$
  - $U, V$ orthogonal/unitary; $\Sigma$ diagonal (singular values)

---

## Rank, Nullity & The Fundamental Theorem

- **Rank:** $\operatorname{rank}(A)$ = dimension of column space.
- **Nullity:** $\operatorname{nullity}(A)$ = dimension of null space.
- **Rank-Nullity Theorem:** $\operatorname{rank}(A) + \operatorname{nullity}(A) = n$ (number of columns)

---

## Advanced Topics

- **Tensor Products & Multilinear Algebra:** Generalize vectors/matrices.
- **Matrix Decompositions:** LU, Cholesky, Schur, Polar
- **Block Matrices & Kronecker Product**
- **Pseudoinverse:** $A^+ = (A^T A)^{-1} A^T$ for full-rank $A$
- **Projection Matrices:** $P = P^2, P = P^T$ (orthogonal projection)
- **Linear Dynamical Systems:** $x_{k+1} = Ax_k$
- **Perron-Frobenius Theorem:** About eigenvalues of positive matrices.
- **Spectral Radius:** $\rho(A) = \max|\lambda_i|$
- **Applications:** Principal Component Analysis (PCA), PageRank, Markov Chains

---


## Advanced Topics Cheat Sheet

### Tensor Products & Multilinear Algebra

- **Tensor Product:** Generalizes the outer product of vectors.If $V$ and $W$ are vector spaces, $V \otimes W$ is the space of all linear combinations of $v \otimes w$.
- **Tensor:** Multidimensional array; rank-$k$ tensor has $k$ indices (e.g., matrix = rank-2, vector = rank-1).
- **Applications:** Physics (stress/strain), data science (multi-way data).

---

### Matrix Decompositions

- **LU Decomposition:** $A = LU$, where $L$ is lower triangular, $U$ is upper triangular.
  - Used to solve systems $Ax = b$ efficiently.
- **Cholesky Decomposition:** For symmetric positive-definite $A$, $A = LL^T$, $L$ lower triangular.
- **Schur Decomposition:** $A = Q T Q^*$, $Q$ unitary, $T$ upper triangular. Every square matrix admits this decomposition.
- **Polar Decomposition:** $A = UP$, $U$ unitary/orthogonal, $P$ positive semi-definite symmetric.

---

### Block Matrices & Kronecker Product

- **Block Matrix:** Matrix composed of smaller matrix "blocks". Useful for structured linear algebra problems.
- **Block Operations:**
  - Block multiplication/addition uses submatrix arithmetic.
- **Kronecker Product:** $A \otimes B$ produces a larger matrix where each element $a_{ij}$ of $A$ is multiplied by the full matrix $B$.
  - If $A$ is $m \times n$, $B$ is $p \times q$, then $A \otimes B$ is $mp \times nq$.
  - Used in tensor algebra, quantum computing, systems theory.

---

### Pseudoinverse

- **Mooreâ€“Penrose Pseudoinverse:** $A^+ = (A^T A)^{-1} A^T$ for full-rank $A$ ($A$ tall). Generalizes matrix inversion to non-square or rank-deficient matrices.
- **Properties:**
  - $A A^+ A = A$
  - $A^+ A A^+ = A^+$
  - Used in least squares, minimum-norm solutions.

---

### Projection Matrices

- **Projection Matrix:** $P = P^2$ (idempotent), $P = P^T$ (symmetric) for orthogonal projection.
- **Orthogonal Projection:** Projects any vector onto a subspace.
  - $P = A(A^T A)^{-1}A^T$ projects onto the column space of $A$.

---

### Linear Dynamical Systems

- **State Update:** $x_{k+1} = A x_k$
- **Solution:** $x_k = A^k x_0$
- **Stability:** Depends on the spectral radius $\rho(A)$ (if $<1$, system is stable).
- **Applications:** Control theory, time series, Markov chains.

---

### Perron-Frobenius Theorem

- **Statement:** For a positive (all entries $>0$) square matrix $A$, there exists a unique largest positive eigenvalue $\lambda_{PF}$ (the Perron root), with a positive eigenvector.
- **Applications:** Population models, Markov processes, ranking algorithms.

---

### Spectral Radius

- **Definition:** $\rho(A) = \max\{|\lambda| : \lambda \text{ is an eigenvalue of } A\}$
- **Importance:** Determines growth/decay of powers $A^k$, stability of systems.

---

### Applications

- **Principal Component Analysis (PCA):**
  - Finds directions of maximal variance in data via eigenvectors of the covariance matrix.
  - Reduces data dimensionality.
- **PageRank:**
  - Ranks web pages using the stationary distribution of a Markov chain defined by the link structure.
- **Markov Chains:**
  - Stochastic matrices, steady-state probabilities found via eigenvectors corresponding to eigenvalue $1$.

---


---



## References

- [MIT OCW: Linear Algebra (Strang)](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/)
- [Axler, Linear Algebra Done Right]
- [Strang, Introduction to Linear Algebra]
- [Paul&#39;s Online Math Notes: Linear Algebra](https://tutorial.math.lamar.edu/)
